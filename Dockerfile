# Bắt đầu từ image Spark 3.5.0 CÓ SẴN Python
FROM apache/spark:3.5.0-python3

# Chuyển sang user 'root' để cài đặt các gói
USER root

# -----------------------------------------------------------------
# BƯỚC 1: Cài đặt thư viện Python (bao gồm MLlib dependencies)
# -----------------------------------------------------------------
RUN pip install --no-cache-dir \
    pandas \
    kafka-python \
    numpy \
    scikit-learn \
    matplotlib \
    seaborn \
    pyarrow

# -----------------------------------------------------------------
# BƯỚC 2: Tải sẵn (Pre-download) các file JARs
# -----------------------------------------------------------------
# Gói Kafka
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar

# Gói MongoDB
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.3.0/mongo-spark-connector_2.12-10.3.0.jar
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.8.2/mongodb-driver-sync-4.8.2.jar
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/mongodb/bson/4.8.2/bson-4.8.2.jar
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.8.2/mongodb-driver-core-4.8.2.jar
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/4.8.2/bson-record-codec-4.8.2.jar

# Gói phụ thuộc chung
RUN wget -P /opt/spark/jars/ https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar

# GraphFrames for graph processing (optional but recommended)
RUN wget -P /opt/spark/jars/ https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar

# -----------------------------------------------------------------
# BƯỚC 3: Copy ALL scripts và file data
# -----------------------------------------------------------------
# Original scripts
COPY streaming_app.py /opt/spark/work-dir/streaming_app.py
COPY streaming_app_k8s.py /opt/spark/work-dir/streaming_app_k8s.py
COPY journey_analysis.py /opt/spark/work-dir/journey_analysis.py

# New advanced scripts
COPY streaming_app_advanced.py /opt/spark/work-dir/streaming_app_advanced.py
COPY batch_analytics_ml.py /opt/spark/work-dir/batch_analytics_ml.py

# Dimension data files
COPY product_catalog.csv /opt/spark/work-dir/product_catalog.csv

# Copy dimension files if they exist (will be generated by generate_dimensions.py)
# Note: These files will be generated before building the Docker image
# If they don't exist yet, run: python generate_dimensions.py

# -----------------------------------------------------------------
# BƯỚC 4: Create necessary directories and set permissions
# -----------------------------------------------------------------
RUN mkdir -p /opt/spark/work-dir/checkpoints && \
    chmod -R 777 /opt/spark/work-dir

# Trả lại quyền cho user 'spark' (user mặc định)
USER $SPARK_UID