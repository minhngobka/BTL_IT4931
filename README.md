# Big Data Customer Journey Analytics

Real-time e-commerce customer journey analytics system using Apache Spark, Kafka, and MongoDB.

## ğŸ“– About This Project

This project demonstrates a **real-time big data analytics pipeline** for analyzing customer behavior in an e-commerce platform. It processes millions of events (views, carts, purchases) in real-time to generate insights about customer journeys, conversion funnels, and behavior patterns.

**Key Features:**
- âš¡ **Real-time streaming** with Apache Spark Structured Streaming
- ğŸ“Š **Complex aggregations** (windowed, stateful, sessionization)
- ğŸ”— **Stream-static joins** with product catalogs
- ğŸ¤– **Machine Learning** (K-Means clustering, Random Forest classification)
- ğŸ¯ **Exactly-once semantics** with checkpointing
- ğŸ“ˆ **Analytics dashboard** via MongoDB queries

## ğŸ—ï¸ Architecture

```
CSV Data (5.3GB) â†’ Kafka â†’ Spark Streaming â†’ MongoDB
                              â†“
                      Batch ML Jobs (6 hours)
```

**Technology Stack:**
- **Apache Spark 3.5.0** - Stream & batch processing
- **Apache Kafka (Strimzi)** - Message queue (3 partitions)
- **MongoDB (Bitnami)** - Analytics database (9 collections)
- **Kubernetes (Minikube)** - Container orchestration
- **Python 3.10+** - Application language
- **Docker** - Containerization

## ğŸš€ Quick Start (5 Steps)

```bash
# 1. Clone the repository
git clone https://github.com/minhngobka/BTL_IT4931.git
cd BTL_IT4931

# 2. Download dataset (5.3GB)
# Get 2019-Oct.csv from: https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store
# Place it in: data/raw/ecommerce_events_2019_oct.csv

# 3. Start Minikube with sufficient resources
minikube start --cpus=4 --memory=8192

# 4. Run automated deployment (installs Kafka, MongoDB, Spark)
./scripts/deploy_all.sh

# 5. Update Kafka broker address and start simulator
export MINIKUBE_IP=$(minikube ip)
sed -i "s|KAFKA_EXTERNAL_BROKER=.*|KAFKA_EXTERNAL_BROKER=$MINIKUBE_IP:31927|" config/.env
python src/utils/event_simulator.py
```

**â±ï¸ Total time:** ~25 minutes

## ğŸ“‹ Prerequisites

Before running the project, ensure you have:

**Required Software:**
- **Docker** (20.10+) 
- **Minikube** (v1.25+)
- **kubectl** (v1.25+)
- **Helm** (v3.0+)
- **Python 3.10+**

**System Requirements:**
- **CPU:** 4 cores minimum
- **RAM:** 8GB minimum
- **Disk:** 20GB free space
- **OS:** Linux (Ubuntu 20.04+)

**Quick Install Commands:**
```bash
# Docker
sudo apt install docker.io
sudo usermod -aG docker $USER

# Minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# kubectl
sudo snap install kubectl --classic

# Helm
sudo snap install helm --classic

# Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸ“ Project Structure

```
bigdata_project/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ streaming/                # Real-time streaming apps
â”‚   â”‚   â”œâ”€â”€ streaming_basic.py
â”‚   â”‚   â”œâ”€â”€ streaming_advanced.py    # â† Main production app
â”‚   â”‚   â””â”€â”€ streaming_kubernetes.py
â”‚   â”œâ”€â”€ batch/                    # Batch processing
â”‚   â”‚   â”œâ”€â”€ ml_analytics.py          # â† ML pipeline
â”‚   â”‚   â””â”€â”€ journey_analysis.py
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ event_simulator.py       # â† Kafka producer
â”‚       â”œâ”€â”€ dimension_generator.py
â”‚       â””â”€â”€ validate_environment.sh
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Raw data (CSV)
â”‚   â””â”€â”€ catalog/                  # Dimension tables
â”œâ”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ .env                      # Local environment
â”‚   â”œâ”€â”€ .env.example              # Template
â”‚   â””â”€â”€ kafka-strimzi.yaml
â”œâ”€â”€ kubernetes/                   # K8s manifests
â”‚   â””â”€â”€ spark-deployments.yaml
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ deploy_all.sh             # â† Run this!
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ SETUP_GUIDE.md
â”‚   â””â”€â”€ TECHNICAL_DOCS.md
â””â”€â”€ Dockerfile
```

## ğŸ¯ What It Does

### Real-Time Analytics

**Input:** E-commerce events (view, cart, purchase)
```json
{
  "event_time": "2019-10-01 00:00:00 UTC",
  "event_type": "purchase",
  "product_id": 3900821,
  "brand": "samsung",
  "price": 489.99,
  "user_id": 554748717,
  "user_session": "9333dfbd-b87a-4708-9857-6336556b0fcc"
}
```

**Processing:**
- Enriches events with product metadata (broadcast join)
- Calculates windowed aggregations (5-min tumbling, 10-min sliding)
- Tracks user sessions with state management
- Analyzes conversion funnels (view â†’ cart â†’ purchase)

**Output:** 9 MongoDB collections with insights
- `enriched_events` - Processed events
- `user_session_analytics` - Session-level metrics
- `conversion_funnel` - Conversion rates
- `event_aggregations` - Time-windowed stats
- `customer_segments` - ML clustering results
- `churn_predictions` - Churn probability scores

### Batch Machine Learning

**K-Means Clustering (4 clusters):**
- Segments customers based on behavior patterns
- Features: purchase frequency, avg order value, session duration

**Random Forest Classification:**
- Predicts customer churn
- 80/20 train-test split
- AUC: 0.75-0.85

## ğŸ”§ Configuration

Edit `config/.env` for your environment:

```env
# Kafka
KAFKA_EXTERNAL_BROKER=192.168.49.2:31927  # Update with minikube ip
KAFKA_TOPIC=customer_events

# MongoDB
MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_DATABASE=bigdata_db

# Simulator
CSV_FILE_PATH=data/raw/ecommerce_events_2019_oct.csv
CHUNK_SIZE=1000
SLEEP_TIME=0.01
```

## ğŸ“Š Monitoring & Verification

### Check Deployment Status

```bash
# Check all pods are running
kubectl get pods

# Expected output:
# my-cluster-kafka-0                Running
# my-cluster-zookeeper-0            Running
# my-mongo-mongodb-0                Running
# spark-streaming-advanced-xxx      Running
```

### Monitor Spark Streaming

```bash
# Port-forward Spark UI
kubectl port-forward deployment/spark-streaming-advanced 4040:4040

# Open in browser: http://localhost:4040
```

### Query MongoDB

```bash
# â­ CÃCH Tá»T NHáº¤T: Query trá»±c tiáº¿p vÃ o pod MongoDB (Ä‘Ã¡ng tin cáº­y 100%)
bash scripts/demo_mongodb.sh

# HOáº¶C query thá»§ cÃ´ng:
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  print('ğŸ“Š Total records:', db.enriched_events.countDocuments());
  db.enriched_events.find().limit(2).forEach(printjson);
"

# Query vá»›i aggregation pipeline
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  db.enriched_events.aggregate([
    {\$match: {event_type: 'view'}},
    {\$group: {_id: '\$product_id', views: {\$sum: 1}}},
    {\$sort: {views: -1}},
    {\$limit: 5}
  ]).forEach(printjson)
"

# ğŸ’¡ LÆ¯U Ã: Port-forward tá»›i localhost cÃ³ thá»ƒ khÃ´ng á»•n Ä‘á»‹nh
# Khuyáº¿n nghá»‹ dÃ¹ng kubectl exec Ä‘á»ƒ query trá»±c tiáº¿p vÃ o pod
```

### Check Kafka

```bash
# List topics
kubectl exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 --list

# Consume messages
kubectl exec -it my-cluster-kafka-0 -- bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic customer_events --from-beginning --max-messages 10
```

## ğŸ› Troubleshooting

### Pods Stuck in Pending/ImagePullBackOff

```bash
# Check pod details
kubectl describe pod <pod-name>

# Reload Docker image to Minikube
docker build -t bigdata-spark:latest .
minikube image load bigdata-spark:latest

# Restart deployment
kubectl rollout restart deployment/spark-streaming-advanced
```

### Kafka Connection Failed

```bash
# Get Minikube IP
minikube ip

# Update .env file
sed -i "s|KAFKA_EXTERNAL_BROKER=.*|KAFKA_EXTERNAL_BROKER=$(minikube ip):31927|" config/.env

# Verify Kafka service
kubectl get svc my-cluster-kafka-external-bootstrap
```

### MongoDB Connection Issues

```bash
# Check MongoDB is running
kubectl get pods | grep mongodb

# â­ Query trá»±c tiáº¿p vÃ o pod (khÃ´ng cáº§n port-forward)
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  db.enriched_events.countDocuments()
"

# Náº¿u váº«n muá»‘n dÃ¹ng port-forward
kubectl port-forward svc/my-mongo-mongodb 27017:27017
kubectl port-forward svc/my-mongo-mongodb 27017:27017

# Test connection
mongosh mongodb://localhost:27017/bigdata_db --eval "db.runCommand({ ping: 1 })"
```

### Event Simulator Not Working

```bash
# Check .env file exists
cat config/.env

# Verify CSV file location
ls -lh data/raw/ecommerce_events_2019_oct.csv

# Test Kafka connectivity
python -c "from kafka import KafkaProducer; print('OK')"
```

## ğŸ§¹ Cleanup

```bash
# Delete all Kubernetes resources
kubectl delete -f kubernetes/spark-deployments.yaml
kubectl delete -f config/kafka-strimzi.yaml

# Or stop Minikube completely
minikube stop
minikube delete
```

## ğŸ“š Detailed Documentation

- **[docs/SETUP_GUIDE.md](docs/SETUP_GUIDE.md)** - Complete step-by-step setup (12 phases)
- **[docs/TECHNICAL_DOCS.md](docs/TECHNICAL_DOCS.md)** - Architecture & technical details
- **[docs/README.md](docs/README.md)** - Detailed feature explanations

## ğŸ“ Academic Context

**Course:** IT4931 - Big Data Analytics  
**Topic:** Real-time Customer Journey Analytics  
**Technologies Demonstrated:**
- Distributed stream processing (Spark Structured Streaming)
- Message queuing (Apache Kafka)
- NoSQL databases (MongoDB)
- Container orchestration (Kubernetes)
- Machine Learning (MLlib)
- Data engineering best practices

## ï¿½ï¿½ Team Members

For teammates cloning this project:
1. Follow the **Quick Start** section above
2. Read `docs/SETUP_GUIDE.md` for detailed explanations
3. Check `config/.env.example` for configuration options

## ğŸ“„ License

Academic project for IT4931 course.

---

**Need help?** Check the troubleshooting section above or see `docs/SETUP_GUIDE.md` for more details.
