# ğŸš€ BÃ i táº­p lá»›n: PhÃ¢n tÃ­ch HÃ nh trÃ¬nh KhÃ¡ch hÃ ng (Customer Journey)

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng Big Data theo kiáº¿n trÃºc Kappa Ä‘á»ƒ phÃ¢n tÃ­ch hÃ nh vi ngÆ°á»i dÃ¹ng trÃªn má»™t trang thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ (view, cart, purchase) theo thá»i gian thá»±c.

## ğŸ› ï¸ YÃªu cáº§u cÃ i Ä‘áº·t (Prerequisites)

TrÆ°á»›c khi báº¯t Ä‘áº§u, báº¡n cáº§n cÃ i Ä‘áº·t cÃ¡c cÃ´ng cá»¥ sau trÃªn mÃ¡y Ubuntu:

1.  **Git:** `sudo apt install git`
2.  **Docker:** [Link hÆ°á»›ng dáº«n cÃ i Docker](https://docs.docker.com/engine/install/ubuntu/)
3.  **Python 3.10+ & Venv:** `sudo apt install python3.10-venv`
4.  **Minikube:** [Link hÆ°á»›ng dáº«n cÃ i Minikube](https://minikube.sigs.k8s.io/docs/start/)
5.  **Kubectl:** `sudo snap install kubectl --classic`
6.  **Helm:** `sudo snap install helm --classic`

## ğŸ“¦ CÃ i Ä‘áº·t dá»± Ã¡n

### 1. Clone Repository

```bash
git clone https://github.com/DucTham2004/bigdata-customer-journey.git
cd bigdata_project
```

### 2. Thiáº¿t láº­p MÃ´i trÆ°á»ng Python

```bash
# Táº¡o mÃ´i trÆ°á»ng áº£o
python3 -m venv venv

# KÃ­ch hoáº¡t mÃ´i trÆ°á»ng
source venv/bin/activate

# CÃ i Ä‘áº·t thÆ° viá»‡n (náº¿u cÃ³ file requirements.txt)
# (Báº¡n cÃ³ thá»ƒ táº¡o file nÃ y báº±ng lá»‡nh: pip freeze > requirements.txt)
pip install pandas kafka-python
```

### 3. Táº£i Dá»¯ liá»‡u (Ráº¥t quan trá»ng)

Do file dá»¯ liá»‡u quÃ¡ lá»›n, nÃ³ khÃ´ng Ä‘Æ°á»£c lÆ°u trÃªn GitHub. Báº¡n cáº§n tá»± táº£i file `2019-Oct.csv` tá»« link Kaggle dÆ°á»›i Ä‘Ã¢y:

* **Link Kaggle:** [https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)

Sau khi táº£i vá», hÃ£y **Ä‘áº·t file `2019-Oct.csv` vÃ o thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n** (ngang hÃ ng vá»›i file `simulator.py`).

## ğŸš€ Khá»Ÿi cháº¡y Háº¡ táº§ng (Giai Ä‘oáº¡n 1)

CÃ¡c lá»‡nh nÃ y chá»‰ cáº§n cháº¡y 1 láº§n Ä‘á»ƒ thiáº¿t láº­p mÃ´i trÆ°á»ng Kubernetes.

### 1. Khá»Ÿi Ä‘á»™ng Minikube

```bash
minikube start --driver=docker --cpus=4 --memory=8g
```

### 2. CÃ i Ä‘áº·t MongoDB

```bash
helm repo add bitnami [https://charts.bitnami.com/bitnami](https://charts.bitnami.com/bitnami)
helm install my-mongo bitnami/mongodb --set auth.enabled=false
```

### 3. CÃ i Ä‘áº·t Strimzi (Kafka Operator)

```bash
helm repo add strimzi [https://strimzi.io/charts/](https://strimzi.io/charts/)
helm install strimzi-operator strimzi/strimzi-kafka-operator
```

### 4. Äá»£i cÃ¡c Operator cháº¡y

DÃ¹ng VSCode má»Ÿ má»™t Terminal má»›i (Ctrl + Shift + \`) vÃ  cháº¡y:
```bash
kubectl get pods -w
```
Äá»£i cho Ä‘áº¿n khi cáº£ `my-mongo-mongodb-...` vÃ  `strimzi-cluster-operator-...` Ä‘á»u `Running`.

### 5. Táº¡o Kafka Cluster (KRaft)

Sau khi operator Ä‘Ã£ cháº¡y, hÃ£y Ã¡p dá»¥ng file cáº¥u hÃ¬nh Kafka cá»§a chÃºng ta:
```bash
kubectl apply -f kafka-combined.yaml
```
Tiáº¿p tá»¥c theo dÃµi `kubectl get pods -w`. Äá»£i cho Ä‘áº¿n khi cÃ¡c pod `my-cluster-kafka-0` vÃ  `my-cluster-entity-operator-...` cÅ©ng `Running`.

---

## ğŸƒ Cháº¡y MÃ´ phá»ng (Data Simulator)

Sau khi toÃ n bá»™ háº¡ táº§ng Ä‘Ã£ `Running`:

### 1. TÃ¬m Ä‘á»‹a chá»‰ Kafka

```bash
# Láº¥y IP cá»§a Minikube
minikube ip

# Láº¥y Cá»•ng (Port) cá»§a Kafka
kubectl get service my-cluster-kafka-external-bootstrap -o=jsonpath='{.spec.ports[0].nodePort}'
```

### 2. Cáº­p nháº­t file `simulator.py`

Má»Ÿ file `simulator.py` vÃ  cáº­p nháº­t dÃ²ng `KAFKA_BROKER` báº±ng IP vÃ  Cá»•ng báº¡n vá»«a tÃ¬m Ä‘Æ°á»£c:

```python
# VÃ­ dá»¥:
KAFKA_BROKER = '192.168.49.2:31234'
```

### 3. Cháº¡y script

(Äáº£m báº£o báº¡n váº«n Ä‘ang trong mÃ´i trÆ°á»ng `venv`)
```bash
python3 simulator.py
```
Báº¡n sáº½ tháº¥y script báº¯t Ä‘áº§u gá»­i dá»¯ liá»‡u lÃªn Kafka.

Giai Ä‘oáº¡n 3: XÃ¢y dá»±ng & Cháº¡y Spark Streaming
Má»Ÿ má»™t terminal má»›i (terminal cÅ© váº«n Ä‘ang cháº¡y simulator).

1. Trá» Terminal vÃ o Docker cá»§a Minikube
ÄÃ¢y lÃ  bÆ°á»›c cá»±c ká»³ quan trá»ng. Do chÃºng ta dÃ¹ng pullPolicy=Never, image pháº£i Ä‘Æ°á»£c build trá»±c tiáº¿p vÃ o bÃªn trong mÃ´i trÆ°á»ng Docker cá»§a Minikube.

Bash

eval $(minikube docker-env)
Terminal cá»§a báº¡n bÃ¢y giá» Ä‘Ã£ káº¿t ná»‘i vá»›i Docker daemon cá»§a Minikube.

2. Build Docker Image
Build image chá»©a á»©ng dá»¥ng Spark, cÃ¡c file JAR vÃ  script Python. (ChÃºng ta dÃ¹ng v1 lÃ m vÃ­ dá»¥).

Bash

docker build -t spark-streaming-app:v1 .
3. Submit á»¨ng dá»¥ng Spark lÃªn Kubernetes
Cháº¡y lá»‡nh spark-submit Ä‘á»ƒ khá»Ÿi Ä‘á»™ng á»©ng dá»¥ng streaming. Lá»‡nh nÃ y sáº½ yÃªu cáº§u Kubernetes táº¡o má»™t pod driver má»›i sá»­ dá»¥ng image chÃºng ta vá»«a build.

Bash

spark-submit \
--master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name customer-journey-streaming \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=default \
--conf spark.kubernetes.container.image=spark-streaming-app:v1 \
--conf spark.kubernetes.container.image.pullPolicy=Never \
local:///opt/spark/work-dir/streaming_app.py
4. Theo dÃµi á»©ng dá»¥ng
Má»Ÿ má»™t terminal thá»© ba Ä‘á»ƒ theo dÃµi cÃ¡c pod.

Bash

kubectl get pods -w
Báº¡n sáº½ tháº¥y pod customer-journey-streaming-...-driver Ä‘Æ°á»£c táº¡o. Náº¿u nÃ³ chuyá»ƒn sang tráº¡ng thÃ¡i Running vÃ  giá»¯ nguyÃªn tráº¡ng thÃ¡i Ä‘Ã³, nghÄ©a lÃ  á»©ng dá»¥ng Ä‘Ã£ cháº¡y thÃ nh cÃ´ng!

Gá»¡ lá»—i:

Náº¿u pod bá»‹ ErrImageNeverPull: Báº¡n Ä‘Ã£ quÃªn cháº¡y eval $(minikube docker-env) trÆ°á»›c khi docker build.

Náº¿u pod chuyá»ƒn sang Error hoáº·c Completed ngay láº­p tá»©c: DÃ¹ng kubectl logs <tÃªn-pod-driver> Ä‘á»ƒ xem lá»—i (thÆ°á»ng lÃ  lá»—i Python hoáº·c lá»—i káº¿t ná»‘i).

ğŸ“Š Giai Ä‘oáº¡n 4: Kiá»ƒm tra Káº¿t quáº£
Náº¿u cáº£ simulator vÃ  pod Spark Ä‘á»u Ä‘ang Running, dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c xá»­ lÃ½ vÃ  lÆ°u vÃ o MongoDB.

CÃ¡ch 1: Sá»­ dá»¥ng CÃ´ng cá»¥ GUI (nhÆ° MongoDB Compass)
TÃ¬m tÃªn pod MongoDB:

Bash

kubectl get pods
(VÃ­ dá»¥: my-mongo-mongodb-54c5b97b6b-b6kld)

Chuyá»ƒn tiáº¿p (port-forward) cá»•ng 27017 cá»§a pod ra mÃ¡y local:

Bash

kubectl port-forward my-mongo-mongodb-54c5b97b6b-b6kld 27017:27017
Má»Ÿ MongoDB Compass vÃ  káº¿t ná»‘i tá»›i mongodb://localhost:27017/.

Báº¡n sáº½ tháº¥y database bigdata_db vÃ  collection customer_events chá»©a Ä‘áº§y dá»¯ liá»‡u.

CÃ¡ch 2: Sá»­ dá»¥ng Command Line (mongosh)
Truy cáº­p shell bÃªn trong pod MongoDB:

Bash

kubectl exec -it my-mongo-mongodb-54c5b97b6b-b6kld -- mongosh
BÃªn trong mongosh, cháº¡y cÃ¡c lá»‡nh sau Ä‘á»ƒ kiá»ƒm tra:

JavaScript

// Chuyá»ƒn sang database
use bigdata_db;

// Äáº¿m sá»‘ lÆ°á»£ng tÃ i liá»‡u
db.customer_events.countDocuments();

// Xem 5 tÃ i liá»‡u máº«u
db.customer_events.find().limit(5);
ğŸ›‘ Dá»«ng Há»‡ thá»‘ng
Sau khi hoÃ n táº¥t, hÃ£y dá»n dáº¹p tÃ i nguyÃªn:

Bash

# 1. Dá»«ng simulator vÃ  spark-submit (Ctrl + C)
# 2. XÃ³a pod Spark (náº¿u nÃ³ váº«n cháº¡y)
kubectl delete pod <tÃªn-pod-driver>

# 3. XÃ³a Kafka
kubectl delete -f kafka-combined.yaml

# 4. Gá»¡ cÃ i Ä‘áº·t Strimzi vÃ  MongoDB
helm uninstall strimzi-operator
helm uninstall my-mongo

# 5. Dá»«ng Minikube
minikube stop
