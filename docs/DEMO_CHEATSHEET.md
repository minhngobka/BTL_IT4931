# ğŸ¯ DEMO CHEATSHEET - In Ra & Äáº·t BÃªn Cáº¡nh

## ğŸ“ GIá»šI THIá»†U 30 GIÃ‚Y

> "Em lÃ m há»‡ thá»‘ng Big Data phÃ¢n tÃ­ch hÃ nh vi khÃ¡ch hÃ ng e-commerce real-time vá»›i:
> - 42 triá»‡u events (5.3GB)
> - Spark Streaming + Kafka + MongoDB
> - Cháº¡y trÃªn Kubernetes"

---

## ğŸ”§ CÃ€I Äáº¶T NHANH (5 phÃºt trÆ°á»›c demo)

```bash
# Terminal 1 - Main
cd ~/bigdata_project
source venv/bin/activate
kubectl get pods  # Táº¥t cáº£ pháº£i Running
```

---

## ğŸ¬ DEMO SCRIPT (10 phÃºt)

### 1ï¸âƒ£ SHOW PROJECT (1 phÃºt)
```bash
tree -L 2
ls -lh data/raw/ecommerce_events_2019_oct.csv  # 5.3GB
```

### 2ï¸âƒ£ START PIPELINE (2 phÃºt)
```bash
# Terminal 2
python src/utils/event_simulator.py
# Chá» tháº¥y: "Sent: view - User: 541312140"
```

### 3ï¸âƒ£ WATCH SPARK (2 phÃºt)
```bash
# Terminal 3
kubectl logs -f deployment/spark-streaming-advanced
# Chá» tháº¥y: "âœ“ Epoch X: Wrote XXX records"
```

### 4ï¸âƒ£ QUERY MONGODB (3 phÃºt)
```bash
# Terminal 4 - CÃ¡ch NHANH vÃ  ÄÃNG TIN Cáº¬Y nháº¥t
bash /tmp/demo_mongodb.sh

# HOáº¶C query thá»§ cÃ´ng:
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  print('ğŸ“Š Records:', db.enriched_events.countDocuments());
  db.enriched_events.find().limit(2).forEach(printjson);
"

# Top 5 products
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  db.enriched_events.aggregate([
    {\$match: {event_type: 'view'}},
    {\$group: {_id: '\$product_id', views: {\$sum: 1}}},
    {\$sort: {views: -1}},
    {\$limit: 5}
  ]).forEach(printjson)
"
```

**ğŸ’¡ TIP:** DÃ¹ng `kubectl exec` thay vÃ¬ port-forward Ä‘á»ƒ demo luÃ´n cháº¡y Ä‘Ãºng!

### 5ï¸âƒ£ BATCH ANALYTICS (2 phÃºt)
```bash
# Terminal 5
python src/batch/journey_analysis.py

# Xem káº¿t quáº£
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  db.funnel_analysis.find().forEach(printjson);
  db.rfm_segments.find().forEach(printjson);
"
```

---

## ğŸ’¬ 10 CÃ‚U Há»I THÆ¯á»œNG Gáº¶P

### 1. "Xá»­ lÃ½ bao nhiÃªu dá»¯ liá»‡u?"
â†’ **42 triá»‡u events (5.3GB), Ä‘Ã£ xá»­ lÃ½ 173K+ events real-time vá»›i tá»‘c Ä‘á»™ 1000 events/giÃ¢y**

### 2. "Táº¡i sao dÃ¹ng Spark Streaming?"
â†’ **Real-time insights, exactly-once semantics, window operations**

### 3. "Xá»­ lÃ½ late data tháº¿ nÃ o?"
â†’ **Watermarking 10 phÃºt**

### 4. "Táº¡i sao MongoDB?"
â†’ **Schema flexible, write nhanh, aggregation máº¡nh**

### 5. "Production throughput?"
â†’ **Hiá»‡n táº¡i: 1K events/s (Ä‘Ã£ test thÃ nh cÃ´ng). Scale lÃªn: 10K+ events/s**

### 6. "KhÃ³ khÄƒn gáº·p pháº£i?"
â†’ **Memory OOM â†’ Broadcast join. Kafka version mismatch â†’ Download Ä‘Ãºng JAR. Port-forward khÃ´ng á»•n Ä‘á»‹nh â†’ kubectl exec**

### 7. "Má»Ÿ rá»™ng nhÆ° nÃ o?"
â†’ **Real-time recommendations, anomaly detection, A/B testing**

### 8. "Code quality?"
â†’ **Project structure chuáº©n, config externalized, Docker, K8s IaC**

### 9. "Performance metrics?"
â†’ **Latency <10s, throughput 1K/s, 173K+ records Ä‘Ã£ xá»­ lÃ½**

### 10. "Há»c Ä‘Æ°á»£c gÃ¬?"
â†’ **Spark streaming, Kafka architecture, K8s orchestration, data pipeline design, troubleshooting distributed systems**

---

## ğŸ“Š KEY NUMBERS (Sá» LIá»†U THá»°C Táº¾)

| Metric | Value |
|--------|-------|
| Dataset | 5.3GB, 42M events |
| **ÄÃ£ xá»­ lÃ½** | **173,707 events** |
| Users | ~5 million |
| Products | ~1 million |
| **Top Product** | **ID 1004856: 1,838 views** |
| **Top User** | **ID 550284046: 4,300 events** |
| Input Rate | 1000 events/sec |
| Latency | <10 seconds |
| Collections | 9 (3 streaming + 6 batch) |
| Kafka Partitions | 3 |
| Spark Resources | 2 cores, 2GB RAM |

---

## ğŸ†˜ TROUBLESHOOTING

### Pod khÃ´ng Running?
```bash
kubectl get pods
kubectl describe pod <pod-name>
kubectl logs <pod-name>
```

### Simulator khÃ´ng connect Ä‘Æ°á»£c Kafka?
```bash
# Check Kafka service
kubectl get svc | grep kafka

# Update config
export MINIKUBE_IP=$(minikube ip)
sed -i "s|KAFKA_EXTERNAL_BROKER=.*|KAFKA_EXTERNAL_BROKER=$MINIKUBE_IP:31927|" config/.env
```

### MongoDB khÃ´ng cÃ³ data?
```bash
# Check Spark logs
kubectl logs deployment/spark-streaming-advanced --tail=30

# Query trá»±c tiáº¿p vÃ o pod (khÃ´ng dÃ¹ng port-forward)
bash scripts/demo_mongodb.sh

# Restart Spark náº¿u cáº§n
kubectl rollout restart deployment/spark-streaming-advanced
```

---

## âœ… CHECKLIST

**30 phÃºt trÆ°á»›c:**
- [ ] Minikube running
- [ ] All pods Running
- [ ] Clear MongoDB (optional)
- [ ] 6 terminals ready
- [ ] Font size lá»›n

**5 phÃºt trÆ°á»›c:**
- [ ] Test simulator
- [ ] Test MongoDB connect
- [ ] Close apps khÃ´ng cáº§n
- [ ] Äá»c láº¡i script 1 láº§n

**Trong khi demo:**
- [ ] NÃ³i cháº­m, rÃµ rÃ ng
- [ ] Giáº£i thÃ­ch táº¡i sao (why), khÃ´ng chá»‰ lÃ m tháº¿ nÃ o (how)
- [ ] Show code quan trá»ng
- [ ] Tá»± tin!

---

## ğŸ¯ Káº¾T LUáº¬N 30 GIÃ‚Y

> "Em Ä‘Ã£ xÃ¢y dá»±ng complete pipeline xá»­ lÃ½ 42M events real-time vá»›i Spark, Kafka, MongoDB trÃªn K8s. 
> System cÃ³ thá»ƒ scale, monitor Ä‘Æ°á»£c, vÃ  apply nhiá»u use cases thá»±c táº¿. 
> Em há»c Ä‘Æ°á»£c distributed computing, stream processing, vÃ  data engineering best practices."

---

**GHI NHá»š:** Breathe, smile, be confident! ğŸš€
