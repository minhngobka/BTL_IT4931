# Big Data Customer Journey Analytics# ğŸš€ BÃ i táº­p lá»›n: PhÃ¢n tÃ­ch HÃ nh trÃ¬nh KhÃ¡ch hÃ ng (Customer Journey)



Real-time customer behavior analysis using **Apache Spark**, **Kafka**, and **MongoDB** on **Kubernetes**.Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng Big Data theo kiáº¿n trÃºc Kappa Ä‘á»ƒ phÃ¢n tÃ­ch hÃ nh vi ngÆ°á»i dÃ¹ng trÃªn má»™t trang thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ (view, cart, purchase) theo thá»i gian thá»±c.



------



## ğŸš€ Quick Start## ï¿½ Quick Links for Team Members



```bash**Choose your path:**

# 1. Clone

git clone https://github.com/minhngobka/BTL_IT4931.git && cd BTL_IT4931- ğŸš€ **[QUICK_CLONE_AND_RUN.md](QUICK_CLONE_AND_RUN.md)** - Fastest setup (copy-paste commands)

- ğŸ“– **[SETUP_FOR_TEAMMATES.md](SETUP_FOR_TEAMMATES.md)** - Complete step-by-step guide with explanations

# 2. Get dataset (download 2019-Oct.csv from Kaggle)- âš™ï¸ **[ENV_SETUP.md](ENV_SETUP.md)** - Environment variables configuration

# https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store- ğŸ”§ **[DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)** - Detailed deployment reference

- ğŸ“Š **[README_ADVANCED.md](README_ADVANCED.md)** - Technical architecture documentation

# 3. Setup

python3 -m venv venv && source venv/bin/activate && pip install -r requirements.txt---



# 4. Deploy## ï¿½ğŸ› ï¸ YÃªu cáº§u cÃ i Ä‘áº·t (Prerequisites)

minikube start --cpus=4 --memory=8192

./run_project_step_by_step.shTrÆ°á»›c khi báº¯t Ä‘áº§u, báº¡n cáº§n cÃ i Ä‘áº·t cÃ¡c cÃ´ng cá»¥ sau trÃªn mÃ¡y Ubuntu:



# 5. Run1.  **Git:** `sudo apt install git`

cp .env.example .env  # Update KAFKA_EXTERNAL_BROKER with minikube ip:port2.  **Docker:** [Link hÆ°á»›ng dáº«n cÃ i Docker](https://docs.docker.com/engine/install/ubuntu/)

python simulator.py3.  **Python 3.10+ & Venv:** `sudo apt install python3.10-venv`

```4.  **Minikube:** [Link hÆ°á»›ng dáº«n cÃ i Minikube](https://minikube.sigs.k8s.io/docs/start/)

5.  **Kubectl:** `sudo snap install kubectl --classic`

**â±ï¸ Time: ~25 minutes** | ğŸ“– **Full guide:** [SETUP_GUIDE.md](SETUP_GUIDE.md)6.  **Helm:** `sudo snap install helm --classic`



---## ğŸ“¦ CÃ i Ä‘áº·t dá»± Ã¡n



## ğŸ“Š What It Does### 1. Clone Repository



Analyzes e-commerce behavior (views, carts, purchases) in real-time:```bash

git clone https://github.com/minhngobka/BTL_IT4931.git

```cd BTL_IT4931

CSV â†’ Kafka â†’ Spark Streaming â†’ MongoDB```

              (enrich, aggregate, ML)

```### 2. Thiáº¿t láº­p MÃ´i trÆ°á»ng Python



**Outputs:**```bash

- ğŸ›ï¸ **20K+ enriched events** (product details + user actions)# Táº¡o mÃ´i trÆ°á»ng áº£o

- ğŸ“ˆ **4K+ window aggregations** (5-min, 10-min windows)python3 -m venv venv

- ğŸ‘¤ **8K+ session analytics** (user journey tracking)

- ğŸ¯ **100+ conversion funnels** (viewâ†’cartâ†’purchase rates)# KÃ­ch hoáº¡t mÃ´i trÆ°á»ng

- ğŸ¤– **ML insights** (K-Means clusters, churn predictions)source venv/bin/activate



---# CÃ i Ä‘áº·t thÆ° viá»‡n (náº¿u cÃ³ file requirements.txt)

# (Báº¡n cÃ³ thá»ƒ táº¡o file nÃ y báº±ng lá»‡nh: pip freeze > requirements.txt)

## ğŸ—ï¸ Architecturepip install pandas kafka-python

```

**Stack:** Spark 3.5 | Kafka (Strimzi) | MongoDB | Kubernetes (Minikube)

### 3. Táº£i Dá»¯ liá»‡u (Ráº¥t quan trá»ng)

```

CSV Simulator â†’ Kafka Topic â†’ Spark Streaming â†’ MongoDB CollectionsDo file dá»¯ liá»‡u quÃ¡ lá»›n, nÃ³ khÃ´ng Ä‘Æ°á»£c lÆ°u trÃªn GitHub. Báº¡n cáº§n tá»± táº£i file `2019-Oct.csv` tá»« link Kaggle dÆ°á»›i Ä‘Ã¢y:

                               â†“ (batch)

                            ML Analytics* **Link Kaggle:** [https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)

```

Sau khi táº£i vá», hÃ£y **Ä‘áº·t file `2019-Oct.csv` vÃ o thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n** (ngang hÃ ng vá»›i file `simulator.py`).

**Key Features:**

- Complex windowed aggregations (tumbling, sliding)## ğŸš€ Khá»Ÿi cháº¡y Háº¡ táº§ng (Giai Ä‘oáº¡n 1)

- Stream-static joins (broadcast)

- Custom UDFs & feature engineeringCÃ¡c lá»‡nh nÃ y chá»‰ cáº§n cháº¡y 1 láº§n Ä‘á»ƒ thiáº¿t láº­p mÃ´i trÆ°á»ng Kubernetes.

- Stateful session tracking

- Exactly-once semantics### 1. Khá»Ÿi Ä‘á»™ng Minikube

- K-Means & Random Forest ML

```bash

ğŸ“– **Details:** [TECHNICAL_DOCS.md](TECHNICAL_DOCS.md)minikube start --driver=docker --cpus=4 --memory=8g

```

---

### 2. CÃ i Ä‘áº·t MongoDB

## ğŸ“ Key Files

```bash

```helm repo add bitnami [https://charts.bitnami.com/bitnami](https://charts.bitnami.com/bitnami)

simulator.py                 # Kafka producerhelm install my-mongo bitnami/mongodb --set auth.enabled=false

streaming_app_advanced.py    # Real-time processing```

batch_analytics_ml.py        # ML analytics

k8s-spark-apps.yaml         # Kubernetes config### 3. CÃ i Ä‘áº·t Strimzi (Kafka Operator)

run_project_step_by_step.sh # Automated deployment

``````bash

helm repo add strimzi [https://strimzi.io/charts/](https://strimzi.io/charts/)

---helm install strimzi-operator strimzi/strimzi-kafka-operator

```

## ğŸ” Quick Commands

### 4. Äá»£i cÃ¡c Operator cháº¡y

```bash

# StatusDÃ¹ng VSCode má»Ÿ má»™t Terminal má»›i (Ctrl + Shift + \`) vÃ  cháº¡y:

kubectl get pods```bash

kubectl get pods -w

# Logs```

kubectl logs -f deployment/spark-streaming-advancedÄá»£i cho Ä‘áº¿n khi cáº£ `my-mongo-mongodb-...` vÃ  `strimzi-cluster-operator-...` Ä‘á»u `Running`.



# MongoDB### 5. Táº¡o Kafka Cluster (KRaft)

kubectl port-forward service/my-mongo-mongodb 27017:27017

mongosh mongodb://localhost:27017 --eval "db.getSiblingDB('bigdata_db').enriched_events.countDocuments()"Sau khi operator Ä‘Ã£ cháº¡y, hÃ£y Ã¡p dá»¥ng file cáº¥u hÃ¬nh Kafka cá»§a chÃºng ta:

```bash

# Spark UIkubectl apply -f kafka-combined.yaml

kubectl port-forward service/spark-streaming-svc 4040:4040  # http://localhost:4040```

Tiáº¿p tá»¥c theo dÃµi `kubectl get pods -w`. Äá»£i cho Ä‘áº¿n khi cÃ¡c pod `my-cluster-kafka-0` vÃ  `my-cluster-entity-operator-...` cÅ©ng `Running`.

# Run batch ML

kubectl create job spark-batch-manual --from=cronjob/spark-batch-ml-scheduled---

```

## ğŸƒ Cháº¡y MÃ´ phá»ng (Data Simulator)

---

Sau khi toÃ n bá»™ háº¡ táº§ng Ä‘Ã£ `Running`:

## ğŸ› ï¸ Prerequisites

### 1. TÃ¬m Ä‘á»‹a chá»‰ Kafka

**Required:**

- Docker (20.0+), Minikube (1.25+), kubectl, Helm (3.0+), Python 3.10+```bash

# Láº¥y IP cá»§a Minikube

**Quick install (Ubuntu):**minikube ip

```bash

sudo apt install docker.io -y && sudo usermod -aG docker $USER# Láº¥y Cá»•ng (Port) cá»§a Kafka

curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64kubectl get service my-cluster-kafka-external-bootstrap -o=jsonpath='{.spec.ports[0].nodePort}'

sudo install minikube-linux-amd64 /usr/local/bin/minikube```

sudo snap install kubectl --classic && sudo snap install helm --classic

```### 2. Cáº­p nháº­t file `simulator.py`



---Má»Ÿ file `simulator.py` vÃ  cáº­p nháº­t dÃ²ng `KAFKA_BROKER` báº±ng IP vÃ  Cá»•ng báº¡n vá»«a tÃ¬m Ä‘Æ°á»£c:



## ğŸ†˜ Troubleshooting```python

# VÃ­ dá»¥:

| Problem | Solution |KAFKA_BROKER = '192.168.49.2:31234'

|---------|----------|```

| Simulator won't connect | Check `.env` has correct `KAFKA_EXTERNAL_BROKER=$(minikube ip):31927` |

| Pods stuck | `minikube delete && minikube start --cpus=4 --memory=8192` |### 3. Cháº¡y script

| Docker build fails | `eval $(minikube docker-env) && docker build -t bigdata-spark:latest .` |

(Äáº£m báº£o báº¡n váº«n Ä‘ang trong mÃ´i trÆ°á»ng `venv`)

**More:** [SETUP_GUIDE.md#troubleshooting](SETUP_GUIDE.md#troubleshooting)```bash

python3 simulator.py

---```

Báº¡n sáº½ tháº¥y script báº¯t Ä‘áº§u gá»­i dá»¯ liá»‡u lÃªn Kafka.

## ğŸ“š Documentation

Giai Ä‘oáº¡n 3: XÃ¢y dá»±ng Docker Image

- **[SETUP_GUIDE.md](SETUP_GUIDE.md)** - Installation & deployment

- **[TECHNICAL_DOCS.md](TECHNICAL_DOCS.md)** - Architecture & features  Má»Ÿ má»™t terminal má»›i (terminal cÅ© váº«n Ä‘ang cháº¡y simulator.py).

- **[.env.example](.env.example)** - Configuration template

1. Trá» Terminal vÃ o Docker cá»§a Minikube

---

ÄÃ¢y lÃ  bÆ°á»›c cá»±c ká»³ quan trá»ng. Image pháº£i Ä‘Æ°á»£c build trá»±c tiáº¿p vÃ o bÃªn trong mÃ´i trÆ°á»ng Docker cá»§a Minikube.

## ğŸ“ Academic Info

eval $(minikube docker-env)

**Project:** Real-time Customer Journey Analytics using Kappa Architecture  

**Demonstrates:** Advanced Spark Streaming, ML integration, Kubernetes deployment

2. Build Docker Image

**Features Coverage:**

âœ… Complex aggregations | âœ… Joins | âœ… UDFs | âœ… State management  Build image chá»©a á»©ng dá»¥ng Spark, cÃ¡c file JAR vÃ  cáº£ 3 script Python. (ChÃºng ta dÃ¹ng v1.0 lÃ m vÃ­ dá»¥).

âœ… Windows | âœ… ML (K-Means, Random Forest) | âœ… Production patterns

docker build -t customer-journey-app:v1.0 .

---



**Ready? Start here:** [SETUP_GUIDE.md](SETUP_GUIDE.md) ğŸš€(LÆ°u Ã½: Báº¡n cÃ³ thá»ƒ Ä‘áº·t tÃªn tag báº¥t ká»³, vÃ­ dá»¥ v15 nhÆ° báº¡n Ä‘Ã£ lÃ m)


âš¡ Giai Ä‘oáº¡n 4: Cháº¡y cÃ¡c Job Spark trÃªn Kubernetes

ChÃºng ta sáº½ submit 3 job Spark song song. Job 1 vÃ  2 lÃ  job Streaming (cháº¡y liÃªn tá»¥c), Job 3 lÃ  job Batch (cháº¡y 1 láº§n rá»“i káº¿t thÃºc).

Job 1: (Streaming) Thu tháº­p dá»¯ liá»‡u thÃ´

Job nÃ y Ä‘á»c tá»« Kafka vÃ  lÆ°u dá»¯ liá»‡u thÃ´ vÃ o collection customer_events.

spark-submit \
--master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name streaming-raw-ingestion \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=default \
--conf spark.kubernetes.container.image=customer-journey-app:v1.0 \
--conf spark.kubernetes.container.image.pullPolicy=Never \
local:///opt/spark/work-dir/streaming_app.py


Job 2: (Streaming) Tá»•ng há»£p dá»¯ liá»‡u (Join + Aggregation)

Job nÃ y Ä‘á»c tá»« Kafka, join vá»›i file CSV, vÃ  lÆ°u káº¿t quáº£ tá»•ng há»£p vÃ o collection event_counts_by_category.

spark-submit \
--master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name streaming-aggregation \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=default \
--conf spark.kubernetes.container.image=customer-journey-app:v1.0 \
--conf spark.kubernetes.container.image.pullPolicy=Never \
local:///opt/spark/work-dir/streaming_app_k8s.py


Job 3: (Batch) PhÃ¢n tÃ­ch HÃ nh trÃ¬nh KhÃ¡ch hÃ ng

Job nÃ y Ä‘á»c toÃ n bá»™ dá»¯ liá»‡u tá»« customer_events (do Job 1 ghi vÃ o), dÃ¹ng Window Functions Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  lÆ°u káº¿t quáº£ phá»…u (funnel) vÃ o collection journey_metrics.

spark-submit \
--master k8s://https://$(minikube ip):8443 \
--deploy-mode cluster \
--name customer-journey-batch \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=default \
--conf spark.kubernetes.container.image=customer-journey-app:v1.0 \
--conf spark.kubernetes.container.image.pullPolicy=Never \
local:///opt/spark/work-dir/journey_analysis.py


4. Theo dÃµi á»©ng dá»¥ng

Má»Ÿ má»™t terminal thá»© ba Ä‘á»ƒ theo dÃµi cÃ¡c pod.

kubectl get pods -w


Báº¡n sáº½ tháº¥y 3 pod driver Ä‘Æ°á»£c táº¡o:

streaming-raw-ingestion-...-driver: Sáº½ á»Ÿ tráº¡ng thÃ¡i Running.

streaming-aggregation-...-driver: Sáº½ á»Ÿ tráº¡ng thÃ¡i Running.

customer-journey-batch-...-driver: Sáº½ chuyá»ƒn sang Running rá»“i Completed.

Gá»¡ lá»—i:

ErrImageNeverPull: Báº¡n Ä‘Ã£ quÃªn cháº¡y eval $(minikube docker-env) trÆ°á»›c khi docker build.

Error / Completed (ngay láº­p tá»©c): DÃ¹ng kubectl logs <tÃªn-pod-driver> Ä‘á»ƒ xem lá»—i.

ğŸ“Š Giai Ä‘oáº¡n 5: Kiá»ƒm tra Káº¿t quáº£

Dá»¯ liá»‡u cá»§a báº¡n bÃ¢y giá» náº±m á»Ÿ 3 collection khÃ¡c nhau trong MongoDB.

1. Káº¿t ná»‘i vá»›i MongoDB

DÃ¹ng MongoDB Compass hoáº·c Command Line.

# Láº¥y tÃªn pod MongoDB
kubectl get pods | grep my-mongo

# Port-forward (thay tÃªn pod cá»§a báº¡n)
kubectl port-forward <my-mongo-mongodb-pod-name> 27017:27017


Má»Ÿ Compass káº¿t ná»‘i tá»›i mongodb://localhost:27017/ vÃ  xem database bigdata_db.

Hoáº·c dÃ¹ng kubectl exec:

# Truy cáº­p shell (thay tÃªn pod cá»§a báº¡n)
kubectl exec -it <my-mongo-mongodb-pod-name> -- mongosh

# BÃªn trong mongosh:
use bigdata_db;


2. Xem cÃ¡c Collection

// 1. Dá»¯ liá»‡u thÃ´ (tá»« Job 1)
db.customer_events.find().limit(5);

// 2. Dá»¯ liá»‡u tá»•ng há»£p (tá»« Job 2)
db.event_counts_by_category.find().limit(5);

// 3. Káº¿t quáº£ phÃ¢n tÃ­ch hÃ nh trÃ¬nh (tá»« Job 3)
db.journey_metrics.find().pretty();


ğŸ›‘ Giai Ä‘oáº¡n 6: Dá»«ng Há»‡ thá»‘ng

Sau khi hoÃ n táº¥t, hÃ£y dá»n dáº¹p tÃ i nguyÃªn:

# 1. Dá»«ng simulator (Ctrl + C)

# 2. XÃ³a cÃ¡c job Spark (Deployment)
# (spark-submit tá»± xÃ³a pod khi deploy-mode=cluster, nhÆ°ng ta nÃªn xÃ³a háº³n app)
# Báº¡n cÃ³ thá»ƒ dÃ¹ng tÃªn app (spark-app-name) hoáº·c tÃªn pod driver Ä‘á»ƒ xÃ³a
kubectl delete pod streaming-raw-ingestion-driver
kubectl delete pod streaming-aggregation-driver
# (Pod 'customer-journey-batch' Ä‘Ã£ 'Completed' nÃªn khÃ´ng cáº§n xÃ³a)

# 3. XÃ³a Kafka
kubectl delete -f kafka-combined.yaml

# 4. Gá»¡ cÃ i Ä‘áº·t Strimzi vÃ  MongoDB
helm uninstall strimzi-operator
helm uninstall my-mongo

# 5. Dá»«ng Minikube
minikube stop
