# ðŸŽ¯ HÆ°á»›ng Dáº«n TrÃ¬nh BÃ y & Demo Dá»± Ãn Big Data

## ðŸ“‹ Má»¤C Lá»¤C
1. [Giá»›i Thiá»‡u Tá»•ng Quan](#giá»›i-thiá»‡u-tá»•ng-quan)
2. [Nhá»¯ng GÃ¬ ÄÃ£ HoÃ n ThÃ nh](#nhá»¯ng-gÃ¬-Ä‘Ã£-hoÃ n-thÃ nh)
3. [Kiáº¿n TrÃºc Há»‡ Thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
4. [Ká»‹ch Báº£n Demo](#ká»‹ch-báº£n-demo)
5. [CÃ¢u Há»i Dá»± Kiáº¿n](#cÃ¢u-há»i-dá»±-kiáº¿n)

---

## ðŸŽ¤ GIá»šI THIá»†U Tá»”NG QUAN (2-3 phÃºt)

### NÃ³i vá»›i giÃ¡o viÃªn:

> **"Em xin phÃ©p trÃ¬nh bÃ y dá»± Ã¡n: Há»‡ Thá»‘ng PhÃ¢n TÃ­ch HÃ nh Vi KhÃ¡ch HÃ ng E-commerce Real-time"**
>
> **Má»¥c tiÃªu:** XÃ¢y dá»±ng má»™t há»‡ thá»‘ng big data xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u khÃ¡ch hÃ ng theo thá»i gian thá»±c, 
> giÃºp doanh nghiá»‡p hiá»ƒu rÃµ hÃ nh vi mua sáº¯m vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh nhanh chÃ³ng.
>
> **Dá»¯ liá»‡u:** 42 triá»‡u sá»± kiá»‡n e-commerce (5.3GB) tá»« Kaggle - ghi nháº­n hÃ nh Ä‘á»™ng xem, thÃªm giá» hÃ ng, 
> vÃ  mua hÃ ng cá»§a khÃ¡ch hÃ ng.
>
> **CÃ´ng nghá»‡:** Apache Spark Structured Streaming, Apache Kafka, MongoDB, Kubernetes trÃªn Minikube.

---

## âœ… NHá»®NG GÃŒ ÄÃƒ HOÃ€N THÃ€NH

### 1. **XÃ¢y Dá»±ng Kiáº¿n TrÃºc Data Pipeline** â­â­â­
- **Data Ingestion:** Kafka vá»›i 3 partitions, replication factor 2
- **Stream Processing:** Spark Structured Streaming vá»›i 3 luá»“ng xá»­ lÃ½ song song
- **Storage:** MongoDB vá»›i 9 collections (3 streaming + 6 batch analytics)
- **Orchestration:** Kubernetes deployment trÃªn Minikube (4 CPUs, 8GB RAM)

### 2. **PhÃ¡t Triá»ƒn á»¨ng Dá»¥ng Streaming** â­â­â­
**File:** `src/streaming/streaming_advanced.py`

**Chá»©c nÄƒng chÃ­nh:**
- âœ… Äá»c dá»¯ liá»‡u real-time tá»« Kafka
- âœ… LÃ m giÃ u dá»¯ liá»‡u vá»›i thÃ´ng tin sáº£n pháº©m vÃ  danh má»¥c
- âœ… PhÃ¢n tÃ­ch hÃ nh vi ngÆ°á»i dÃ¹ng theo session (30 phÃºt timeout)
- âœ… TÃ­nh toÃ¡n metrics aggregation (5s tumbling window)
- âœ… Ghi káº¿t quáº£ vÃ o MongoDB vá»›i 3 collections:
  - `enriched_events`: Sá»± kiá»‡n Ä‘Ã£ Ä‘Æ°á»£c lÃ m giÃ u
  - `event_aggregations`: Metrics tá»•ng há»£p theo event_type
  - `user_session_analytics`: PhÃ¢n tÃ­ch session ngÆ°á»i dÃ¹ng

**Ká»¹ thuáº­t sá»­ dá»¥ng:**
- Window operations (tumbling, session)
- Stream-stream joins vá»›i broadcast
- Watermarking cho late data handling
- Stateful processing vá»›i sessionization

### 3. **PhÃ¡t Triá»ƒn á»¨ng Dá»¥ng Batch Analytics** â­â­â­
**File:** `src/batch/journey_analysis.py`

**PhÃ¢n tÃ­ch nÃ¢ng cao:**
- âœ… Customer Journey Analysis: PhÃ¢n tÃ­ch hÃ nh trÃ¬nh mua hÃ ng
- âœ… Funnel Analysis: Tá»· lá»‡ chuyá»ƒn Ä‘á»•i view â†’ cart â†’ purchase
- âœ… RFM Segmentation: PhÃ¢n khÃºc khÃ¡ch hÃ ng (Recency, Frequency, Monetary)
- âœ… Category Performance: PhÃ¢n tÃ­ch hiá»‡u suáº¥t theo danh má»¥c
- âœ… Time-based Patterns: PhÃ¢n tÃ­ch theo giá», ngÃ y trong tuáº§n
- âœ… Product Recommendations: Gá»£i Ã½ sáº£n pháº©m dá»±a trÃªn co-occurrence

**File:** `src/batch/ml_analytics.py`
- âœ… K-Means Clustering: PhÃ¢n nhÃ³m khÃ¡ch hÃ ng
- âœ… Predictive Analytics: Dá»± Ä‘oÃ¡n purchase probability

### 4. **Data Engineering Best Practices** â­â­
- âœ… Cáº¥u trÃºc dá»± Ã¡n chuyÃªn nghiá»‡p (src/, data/, config/, kubernetes/, scripts/)
- âœ… Dimension tables: Product catalog, category hierarchy, user dimension
- âœ… Configuration management vá»›i .env files
- âœ… Docker containerization
- âœ… Automated deployment scripts

### 5. **Monitoring & Observability** â­
- âœ… Spark UI Ä‘á»ƒ theo dÃµi streaming jobs
- âœ… MongoDB queries Ä‘á»ƒ verify data
- âœ… Kubernetes dashboard Ä‘á»ƒ monitor resources
- âœ… Structured logging

---

## ðŸ—ï¸ KIáº¾N TRÃšC Há»† THá»NG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Dataset   â”‚  5.3GB, 42M events
â”‚  (2019-Oct.csv) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Simulator â”‚  Python script
â”‚  (Kafka Producer)â”‚  â†’ Topic: ecommerce-events
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Apache Kafka (Strimzi)              â”‚
â”‚  â€¢ 1 broker, 3 partitions, RF=2             â”‚
â”‚  â€¢ Topic: ecommerce-events                  â”‚
â”‚  â€¢ External access: NodePort 31927          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache Spark Structured Streaming         â”‚
â”‚  â€¢ Driver: 2 cores, 2GB RAM                 â”‚
â”‚  â€¢ Executor: 2 cores, 2GB RAM               â”‚
â”‚  â€¢ 3 parallel streams:                      â”‚
â”‚    1. Enriched Events                       â”‚
â”‚    2. Event Aggregations                    â”‚
â”‚    3. User Session Analytics                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            MongoDB (Bitnami)                 â”‚
â”‚  Database: bigdata_db                        â”‚
â”‚                                              â”‚
â”‚  Streaming Collections:                     â”‚
â”‚  â€¢ enriched_events                          â”‚
â”‚  â€¢ event_aggregations                       â”‚
â”‚  â€¢ user_session_analytics                   â”‚
â”‚                                              â”‚
â”‚  Batch Analytics Collections:               â”‚
â”‚  â€¢ customer_journey                         â”‚
â”‚  â€¢ funnel_analysis                          â”‚
â”‚  â€¢ rfm_segments                             â”‚
â”‚  â€¢ category_performance                     â”‚
â”‚  â€¢ time_patterns                            â”‚
â”‚  â€¢ product_recommendations                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

           All running on
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Kubernetes (Minikube)  â”‚
    â”‚   4 CPUs, 8GB RAM        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¬ Ká»ŠCH Báº¢N DEMO (10-15 phÃºt)

### **CHUáº¨N Bá»Š TRÆ¯á»šC KHI DEMO:**

```bash
# Terminal 1: Chuáº©n bá»‹
cd ~/bigdata_project
source venv/bin/activate

# Kiá»ƒm tra há»‡ thá»‘ng
kubectl get pods                    # Táº¥t cáº£ pods pháº£i Running
minikube status                     # Pháº£i Running
```

---

### **PHáº¦N 1: GIá»šI THIá»†U Cáº¤U TRÃšC Dá»° ÃN** (2 phÃºt)

```bash
# Hiá»ƒn thá»‹ cáº¥u trÃºc dá»± Ã¡n
tree -L 2 -d

# Hoáº·c
ls -la
ls -la src/
ls -la data/
ls -la kubernetes/
```

**Giáº£i thÃ­ch:**
> "Em Ä‘Ã£ tá»• chá»©c dá»± Ã¡n theo cáº¥u trÃºc chuáº©n data engineering:
> - `src/`: MÃ£ nguá»“n chia thÃ nh streaming, batch, vÃ  utilities
> - `data/`: Dá»¯ liá»‡u thÃ´ vÃ  dimension tables
> - `config/`: Cáº¥u hÃ¬nh Kafka, MongoDB, environment variables
> - `kubernetes/`: Kubernetes manifests cho deployment
> - `scripts/`: Automation scripts"

---

### **PHáº¦N 2: DEMO DATA PIPELINE REAL-TIME** (5-6 phÃºt)

#### **BÆ°á»›c 1: Kiá»ƒm tra infrastructure**

```bash
# Terminal 1: Xem tráº¡ng thÃ¡i pods
kubectl get pods

# Giáº£i thÃ­ch tá»«ng component
kubectl get pods | grep mongo        # MongoDB
kubectl get pods | grep kafka        # Kafka broker
kubectl get pods | grep spark        # Spark streaming
```

**NÃ³i:**
> "Há»‡ thá»‘ng gá»“m 3 components chÃ­nh Ä‘ang cháº¡y trÃªn Kubernetes:
> - MongoDB: NoSQL database Ä‘á»ƒ lÆ°u káº¿t quáº£
> - Kafka: Message queue vá»›i 3 partitions
> - Spark: Streaming engine Ä‘á»ƒ xá»­ lÃ½ real-time"

#### **BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng Event Simulator**

```bash
# Terminal 2: Má»Ÿ terminal má»›i
cd ~/bigdata_project
source venv/bin/activate

# Cháº¡y simulator
python src/utils/event_simulator.py
```

**NÃ³i:**
> "ÄÃ¢y lÃ  Event Simulator - Ä‘á»c dá»¯ liá»‡u tá»« CSV 5.3GB vÃ  gá»­i vÃ o Kafka 
> nhÆ° thá»ƒ khÃ¡ch hÃ ng Ä‘ang tÆ°Æ¡ng tÃ¡c thá»±c táº¿. Má»—i giÃ¢y gá»­i 1000 events."

**Quan sÃ¡t output:**
```
>>> Káº¿t ná»‘i Kafka THÃ€NH CÃ”NG!
Báº¯t Ä‘áº§u Ä‘á»c file: data/raw/ecommerce_events_2019_oct.csv
--- Gá»­i 1000 sá»± kiá»‡n ---
Sent: view - User: 541312140
Sent: purchase - User: 514591159
Sent: cart - User: 550121407
...
```

#### **BÆ°á»›c 3: Theo dÃµi Spark Processing**

```bash
# Terminal 3: Má»Ÿ terminal má»›i
kubectl logs -f deployment/spark-streaming-advanced
```

**NÃ³i:**
> "Spark Ä‘ang nháº­n dá»¯ liá»‡u tá»« Kafka, xá»­ lÃ½ vÃ  ghi vÃ o MongoDB.
> Má»—i batch ghi hÃ ng trÄƒm records vÃ o 3 collections khÃ¡c nhau."

**Quan sÃ¡t output:**
```
âœ“ Epoch 1: Wrote 789 records to enriched_events
âœ“ Epoch 1: Wrote 217 records to event_aggregations
âœ“ Epoch 1: Wrote 311 records to user_session_analytics
```

#### **BÆ°á»›c 4: Truy váº¥n MongoDB Real-time**

```bash
# Terminal 4: Query MongoDB trá»±c tiáº¿p trong pod (cÃ¡ch Ä‘Ã¡ng tin cáº­y nháº¥t)
bash /tmp/demo_mongodb.sh

# HOáº¶C query thá»§ cÃ´ng:
# 1. Äáº¿m sá»‘ records
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  print('ðŸ“Š RECORD COUNTS:');
  print('enriched_events:', db.enriched_events.countDocuments());
  print('event_aggregations:', db.event_aggregations.countDocuments());
  print('user_sessions:', db.user_session_analytics.countDocuments());
"

# 2. Xem dá»¯ liá»‡u máº«u
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  db.enriched_events.find().limit(2).forEach(printjson)
"

# 3. Top 5 products Ä‘Æ°á»£c xem nhiá»u nháº¥t
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  db.enriched_events.aggregate([
    {\$match: {event_type: 'view'}},
    {\$group: {_id: '\$product_id', views: {\$sum: 1}}},
    {\$sort: {views: -1}},
    {\$limit: 5}
  ]).forEach(printjson)
"

# 4. Top 5 users cÃ³ nhiá»u hÃ nh Ä‘á»™ng nháº¥t
kubectl exec deployment/my-mongo-mongodb -- mongosh bigdata_db --quiet --eval "
  db.user_session_analytics.find().sort({total_events: -1}).limit(5).forEach(printjson)
"
```

**ðŸ’¡ LÆ°u Ã½ quan trá»ng:**
- Spark ghi vÃ o MongoDB qua **internal Kubernetes service**: `my-mongo-mongodb.default.svc.cluster.local:27017`
- Port-forward tá»›i localhost cÃ³ thá»ƒ khÃ´ng á»•n Ä‘á»‹nh trong demo
- **Khuyáº¿n nghá»‹**: DÃ¹ng `kubectl exec` Ä‘á»ƒ query trá»±c tiáº¿p vÃ o pod MongoDB (Ä‘Ã¡ng tin cáº­y 100%)

**NÃ³i:**
> "Dá»¯ liá»‡u Ä‘ang Ä‘Æ°á»£c ghi vÃ o MongoDB trong real-time. 
> - enriched_events: Má»—i sá»± kiá»‡n Ä‘Æ°á»£c lÃ m giÃ u vá»›i thÃ´ng tin sáº£n pháº©m, category
> - event_aggregations: Tá»•ng há»£p sá»‘ lÆ°á»£ng view, cart, purchase má»—i 5 giÃ¢y
> - user_session_analytics: PhÃ¢n tÃ­ch session cá»§a tá»«ng user (timeout 30 phÃºt)
> 
> Em query trá»±c tiáº¿p vÃ o MongoDB pod Ä‘á»ƒ Ä‘áº£m báº£o demo luÃ´n cháº¡y Ä‘Ãºng."

#### **BÆ°á»›c 5: Spark UI**

```bash
# Terminal 6: Port forward Spark UI
kubectl port-forward deployment/spark-streaming-advanced 4040:4040
```

**Má»Ÿ browser: http://localhost:4040**

**NÃ³i:**
> "Spark UI cho phÃ©p monitor streaming job:
> - Streaming tab: Input rate, processing time, batch duration
> - SQL tab: Physical plans cá»§a cÃ¡c queries
> - Executors tab: Resource usage"

**Chá»‰ vÃ o:**
- Input Rate: X events/sec
- Processing Time: ~Y seconds
- Batch Duration: 30 seconds

---

### **PHáº¦N 3: DEMO BATCH ANALYTICS** (3-4 phÃºt)

#### **Cháº¡y Customer Journey Analysis**

```bash
# Terminal 7: Trong virtual environment
cd ~/bigdata_project
source venv/bin/activate

# Kiá»ƒm tra cÃ³ Ä‘á»§ dá»¯ liá»‡u chÆ°a (cáº§n Ã­t nháº¥t 1000 events)
mongosh mongodb://localhost:27017/bigdata_db --eval "db.enriched_events.countDocuments()"

# Cháº¡y batch analytics
python src/batch/journey_analysis.py
```

**NÃ³i:**
> "Batch analytics cháº¡y trÃªn dá»¯ liá»‡u Ä‘Ã£ tÃ­ch lÅ©y Ä‘á»ƒ phÃ¢n tÃ­ch sÃ¢u hÆ¡n:
> - Customer Journey: HÃ nh trÃ¬nh tá»« view â†’ cart â†’ purchase
> - Funnel Analysis: Tá»· lá»‡ chuyá»ƒn Ä‘á»•i á»Ÿ má»—i bÆ°á»›c
> - RFM Segmentation: PhÃ¢n khÃºc khÃ¡ch hÃ ng thÃ nh Champions, Loyal, At Risk...
> - Category Performance: Danh má»¥c nÃ o bÃ¡n cháº¡y nháº¥t
> - Time Patterns: Giá» nÃ o trong ngÃ y cÃ³ traffic cao nháº¥t"

#### **Xem káº¿t quáº£**

```bash
# Xem cÃ¡c collections má»›i
mongosh mongodb://localhost:27017/bigdata_db

# Funnel Analysis
db.funnel_analysis.find().pretty()

# RFM Segments
db.rfm_segments.find().pretty()

# Category Performance
db.category_performance.find().sort({total_revenue: -1}).limit(5).pretty()

# Time Patterns
db.time_patterns.find().sort({hour_of_day: 1}).pretty()

# Product Recommendations
db.product_recommendations.find().limit(3).pretty()
```

**Giáº£i thÃ­ch káº¿t quáº£:**
> "VÃ­ dá»¥ Funnel Analysis cho tháº¥y:
> - 100% users view sáº£n pháº©m
> - 5% thÃªm vÃ o giá» hÃ ng  
> - 2% hoÃ n táº¥t mua hÃ ng
> â†’ Cáº§n cáº£i thiá»‡n conversion rate á»Ÿ bÆ°á»›c cart"

---

### **PHáº¦N 4: DEMO KUBERNETES ORCHESTRATION** (2 phÃºt)

```bash
# Xem táº¥t cáº£ resources
kubectl get all

# Xem chi tiáº¿t deployment
kubectl describe deployment spark-streaming-advanced

# Xem resource usage
kubectl top pods

# Scale up/down (náº¿u muá»‘n show)
kubectl scale deployment spark-streaming-advanced --replicas=2
kubectl get pods -w  # Xem pod má»›i start

# Scale back
kubectl scale deployment spark-streaming-advanced --replicas=1
```

**NÃ³i:**
> "Kubernetes giÃºp quáº£n lÃ½ vÃ  scale há»‡ thá»‘ng:
> - Auto-restart náº¿u pod bá»‹ lá»—i
> - Resource limits Ä‘á»ƒ trÃ¡nh overload
> - CÃ³ thá»ƒ scale theo nhu cáº§u"

---

## â“ CÃ‚U Há»ŽI Dá»± KIáº¾N & CÃCH TRáº¢ Lá»œI

### **1. "Em xá»­ lÃ½ bao nhiÃªu dá»¯ liá»‡u?"**

**Tráº£ lá»i:**
> "Em xá»­ lÃ½ 42 triá»‡u events (5.3GB) tá»« dataset Kaggle vá» e-commerce. 
> Dataset nÃ y ghi nháº­n hÃ nh vi cá»§a khoáº£ng 5 triá»‡u users trÃªn 1 triá»‡u sáº£n pháº©m 
> trong thÃ¡ng 10/2019. Event Simulator Ä‘á»c vÃ  gá»­i vÃ o Kafka vá»›i tá»‘c Ä‘á»™ 1000 events/giÃ¢y, 
> tÆ°Æ¡ng Ä‘Æ°Æ¡ng ~11 giá» cháº¡y liÃªn tá»¥c Ä‘á»ƒ háº¿t dá»¯ liá»‡u."

### **2. "Táº¡i sao chá»n Spark Streaming thay vÃ¬ Spark Batch?"**

**Tráº£ lá»i:**
> "Em chá»n Spark Structured Streaming vÃ¬:
> 1. **Real-time insights:** Doanh nghiá»‡p cáº§n biáº¿t ngay khÃ¡ch hÃ ng Ä‘ang lÃ m gÃ¬ Ä‘á»ƒ pháº£n á»©ng ká»‹p thá»i
> 2. **Event-driven architecture:** TÃ­ch há»£p tá»‘t vá»›i Kafka
> 3. **Micro-batch processing:** CÃ¢n báº±ng giá»¯a latency vÃ  throughput
> 4. **Exactly-once semantics:** Äáº£m báº£o dá»¯ liá»‡u khÃ´ng bá»‹ duplicate hay máº¥t mÃ¡t
> 5. **Window operations:** Há»— trá»£ sessionization vÃ  time-based aggregations
> 
> Batch analytics váº«n Ä‘Æ°á»£c dÃ¹ng cho phÃ¢n tÃ­ch sÃ¢u hÆ¡n nhÆ° RFM, clustering, recommendations."

### **3. "LÃ m tháº¿ nÃ o xá»­ lÃ½ late data?"**

**Tráº£ lá»i:**
> "Em sá»­ dá»¥ng Watermarking trong Spark Structured Streaming:
> ```python
> df = df.withWatermark('event_time', '10 minutes')
> ```
> Events Ä‘áº¿n muá»™n trong vÃ²ng 10 phÃºt váº«n Ä‘Æ°á»£c xá»­ lÃ½. 
> Sau 10 phÃºt, state cÅ© sáº½ Ä‘Æ°á»£c clean up Ä‘á»ƒ trÃ¡nh memory leak."

### **4. "Táº¡i sao dÃ¹ng MongoDB thay vÃ¬ relational database?"**

**Tráº£ lá»i:**
> "MongoDB phÃ¹ há»£p vÃ¬:
> 1. **Schema flexibility:** Event data cÃ³ thá»ƒ thay Ä‘á»•i cáº¥u trÃºc theo thá»i gian
> 2. **Write performance:** Ghi nhanh vá»›i batch inserts
> 3. **JSON format:** Match vá»›i Spark DataFrame schema
> 4. **Aggregation framework:** Query máº¡nh máº½ cho analytics
> 5. **Horizontal scaling:** Dá»… scale vá»›i sharding khi data lá»›n"

### **5. "Há»‡ thá»‘ng cÃ³ handle Ä‘Æ°á»£c production load khÃ´ng?"**

**Tráº£ lá»i:**
> "Vá»›i setup hiá»‡n táº¡i trÃªn Minikube (4 CPUs, 8GB RAM):
> - Throughput: ~1000 events/giÃ¢y
> - Latency: ~5-10 giÃ¢y end-to-end
> 
> Äá»ƒ production, cáº§n:
> 1. **Scale Kafka:** ThÃªm brokers, tÄƒng partitions (recommend 1 partition per CPU core)
> 2. **Scale Spark:** TÄƒng executors vÃ  memory (recommend 3 executors, 4GB/executor)
> 3. **Scale MongoDB:** Sharding vÃ  replication
> 4. **Use managed services:** AWS MSK (Kafka), EMR (Spark), DocumentDB (MongoDB)
> 
> Vá»›i config Ä‘Ã³ cÃ³ thá»ƒ xá»­ lÃ½ 10,000+ events/giÃ¢y."

### **6. "Em gáº·p khÃ³ khÄƒn gÃ¬ khi lÃ m project?"**

**Tráº£ lá»i (chá»n 2-3 Ä‘iá»ƒm):**
> "Em gáº·p má»™t sá»‘ thÃ¡ch thá»©c:
> 
> 1. **Memory management:** Ban Ä‘áº§u Spark bá»‹ OOM khi join vá»›i product catalog. 
>    ÄÃ£ giáº£i quyáº¿t báº±ng broadcast join cho dimension tables nhá».
> 
> 2. **Sessionization:** XÃ¡c Ä‘á»‹nh session boundary phá»©c táº¡p. ÄÃ£ dÃ¹ng session window 
>    vá»›i 30 phÃºt timeout, phÃ¹ há»£p vá»›i user behavior.
> 
> 3. **Kafka connector:** Spark-Kafka integration cáº§n Ä‘Ãºng version dependencies. 
>    ÄÃ£ download Ä‘Ãºng JAR files cho Spark 3.5.0 vÃ  Kafka 3.4.0.
> 
> 4. **Docker build:** Container size lá»›n (~2GB). ÄÃ£ optimize báº±ng multi-stage build 
>    vÃ  chá»‰ copy files cáº§n thiáº¿t."

### **7. "CÃ³ thá»ƒ má»Ÿ rá»™ng project nÃ y nhÆ° tháº¿ nÃ o?"**

**Tráº£ lá»i:**
> "Em cÃ³ má»™t sá»‘ Ã½ tÆ°á»Ÿng má»Ÿ rá»™ng:
> 
> 1. **Real-time recommendations:** DÃ¹ng ALS collaborative filtering trong streaming
> 2. **Anomaly detection:** Machine learning Ä‘á»ƒ phÃ¡t hiá»‡n fraud, bot traffic
> 3. **A/B testing framework:** So sÃ¡nh different user experiences
> 4. **Real-time dashboard:** Grafana + Prometheus Ä‘á»ƒ visualize metrics
> 5. **CDC (Change Data Capture):** Sync vá»›i operational databases
> 6. **Lambda architecture:** Combine streaming vÃ  batch vá»›i Delta Lake
> 7. **Multi-tenant:** Support nhiá»u e-commerce sites trÃªn cÃ¹ng platform"

### **8. "Code quality nhÆ° tháº¿ nÃ o?"**

**Tráº£ lá»i:**
> "Em focus vÃ o code quality:
> 
> 1. **Project structure:** Tá»• chá»©c theo best practices (src/, data/, config/)
> 2. **Configuration management:** Externalize vá»›i .env files
> 3. **Error handling:** Try-catch vÃ  logging Ä‘áº§y Ä‘á»§
> 4. **Documentation:** README, SETUP_GUIDE, TECHNICAL_DOCS
> 5. **Containerization:** Docker cho reproducibility
> 6. **IaC:** Kubernetes manifests cho infrastructure as code
> 7. **Version control:** Git vá»›i meaningful commit messages"

### **9. "Performance metrics lÃ  gÃ¬?"**

**Tráº£ lá»i:**
> "Em Ä‘o cÃ¡c metrics sau:
> 
> **Streaming:**
> - Input rate: 1000 events/sec
> - Processing time: 5-8 seconds per batch
> - End-to-end latency: <10 seconds
> - Records processed: ~30,000 per batch (30s window)
> 
> **Batch:**
> - Journey analysis: 10M events trong ~5 phÃºt
> - K-means clustering: 100K users trong ~2 phÃºt
> 
> **Storage:**
> - MongoDB: 3GB sau 1 triá»‡u events
> - Compression ratio: ~60% (raw CSV vs MongoDB)"

### **10. "Em há»c Ä‘Æ°á»£c gÃ¬ tá»« project nÃ y?"**

**Tráº£ lá»i:**
> "Em há»c Ä‘Æ°á»£c ráº¥t nhiá»u:
> 
> 1. **Technical skills:**
>    - Spark Structured Streaming vá»›i window operations
>    - Kafka architecture vÃ  partitioning strategy
>    - Kubernetes orchestration vÃ  resource management
>    - MongoDB aggregation framework
> 
> 2. **Data engineering principles:**
>    - Data pipeline design patterns
>    - Stream vs batch processing tradeoffs
>    - Schema evolution vÃ  backward compatibility
>    - Monitoring vÃ  debugging distributed systems
> 
> 3. **Soft skills:**
>    - Äá»c documentation ká»¹ (Spark, Kafka, K8s)
>    - Debug vÃ  troubleshoot complex issues
>    - Tá»• chá»©c code theo best practices
>    - Technical writing (documentation)
> 
> 4. **Business understanding:**
>    - E-commerce metrics (conversion rate, funnel analysis)
>    - Customer segmentation (RFM model)
>    - Real-time vs batch analytics use cases"

---

## ðŸ“Š KEY METRICS Äá»‚ SHOW

Chuáº©n bá»‹ sáºµn cÃ¡c sá»‘ liá»‡u nÃ y:

```bash
# 1. Dataset size
ls -lh data/raw/ecommerce_events_2019_oct.csv
# â†’ 5.3GB, 42 million events

# 2. Number of collections
mongosh --eval "db.adminCommand('listDatabases')" | grep bigdata_db

# 3. Records in each collection
mongosh mongodb://localhost:27017/bigdata_db --eval "
  db.getCollectionNames().forEach(function(col) {
    print(col + ': ' + db[col].countDocuments())
  })
"

# 4. Infrastructure resources
kubectl top pods

# 5. Processing rate
# Xem trong Spark UI: Input Rate graph
```

---

## ðŸŽ¯ TIPS CHO BÃ€I TRÃŒNH BÃ€Y

### **âœ… NÃŠN:**

1. **Tá»± tin vÃ  rÃµ rÃ ng:** NÃ³i cháº­m, rÃµ rÃ ng, maintain eye contact
2. **Demo trá»±c tiáº¿p:** Cháº¡y tháº­t há»‡ thá»‘ng, khÃ´ng dÃ¹ng slides quÃ¡ nhiá»u
3. **Explain WHY:** Giáº£i thÃ­ch táº¡i sao chá»n cÃ´ng nghá»‡ Ä‘Ã³, khÃ´ng chá»‰ HOW
4. **Show code quan trá»ng:** Má»Ÿ file Python, chá»‰ vÃ o cÃ¡c Ä‘oáº¡n code hay
5. **CÃ³ backup plan:** Náº¿u demo fail, cÃ³ screenshots/video sáºµn
6. **Interactive:** Há»i giÃ¡o viÃªn cÃ³ muá»‘n tháº¥y gÃ¬ thÃªm khÃ´ng

### **âŒ KHÃ”NG NÃŠN:**

1. **Äá»c slides:** Biáº¿t ná»™i dung, khÃ´ng cáº§n Ä‘á»c
2. **NÃ³i quÃ¡ technical:** Balance giá»¯a technical depth vÃ  clarity
3. **Che giáº¥u háº¡n cháº¿:** ThÃ nh tháº­t vá» limitations vÃ  cÃ¡ch improve
4. **Rush:** LÃ m cháº­m, Ä‘áº£m báº£o má»i bÆ°á»›c cháº¡y Ä‘Ãºng
5. **Phá»¥ thuá»™c internet:** Download sáºµn dependencies

---

## ðŸ“¸ SCREENSHOTS Cáº¦N CHUáº¨N Bá»Š (BACKUP)

1. **Project structure:** `tree` output
2. **Pods running:** `kubectl get pods` output
3. **Event Simulator:** Console output sending events
4. **Spark logs:** Processing batches successfully
5. **MongoDB data:** Sample documents tá»« má»—i collection
6. **Spark UI:** Streaming tab showing input rate graph
7. **Batch results:** Funnel analysis, RFM segments
8. **Kubernetes dashboard:** Resource usage graphs

---

## â±ï¸ TIMELINE Tá»”NG THá»‚

- **00:00-02:00** - Giá»›i thiá»‡u project vÃ  objectives
- **02:00-04:00** - Show project structure vÃ  architecture
- **04:00-09:00** - Demo real-time pipeline (simulator â†’ Spark â†’ MongoDB)
- **09:00-12:00** - Demo batch analytics vÃ  results
- **12:00-14:00** - Show Kubernetes orchestration
- **14:00-15:00** - Tá»•ng káº¿t vÃ  Q&A

**TOTAL: 15 phÃºt demo + 5-10 phÃºt Q&A = ~25 phÃºt**

---

## ðŸš€ CHECKLIST TRÆ¯á»šC KHI DEMO

```bash
# 1 ngÃ y trÆ°á»›c:
â–¡ Test toÃ n bá»™ pipeline tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i
â–¡ Chá»¥p screenshots backup
â–¡ Viáº¿t script demo (file nÃ y)
â–¡ Practice 2-3 láº§n

# 1 giá» trÆ°á»›c:
â–¡ Start Minikube: minikube start
â–¡ Check all pods running: kubectl get pods
â–¡ Activate venv: source venv/bin/activate
â–¡ Open 6 terminals sáºµn, label rÃµ rÃ ng
â–¡ Clear MongoDB Ä‘á»ƒ cÃ³ data má»›i: mongosh â†’ db.dropDatabase()

# 5 phÃºt trÆ°á»›c:
â–¡ Restart Spark deployment Ä‘á»ƒ clean state
â–¡ Test network connectivity
â–¡ Close unnecessary applications
â–¡ Set terminal font size lá»›n (cho dá»… nhÃ¬n)
```

---

## ðŸ’¡ Máº¸O CUá»I CÃ™NG

1. **Storytelling:** Ká»ƒ nhÆ° má»™t cÃ¢u chuyá»‡n - tá»« problem â†’ solution â†’ results
2. **Be enthusiastic:** Show passion vá» project
3. **Admit unknowns:** Náº¿u khÃ´ng biáº¿t cÃ¢u tráº£ lá»i, thÃ nh tháº­t "Em sáº½ research thÃªm"
4. **Connect to real-world:** NÃ³i vá» use cases thá»±c táº¿ (Amazon, Shopee, Tiki)
5. **Highlight learning:** Focus vÃ o nhá»¯ng gÃ¬ Ä‘Ã£ há»c Ä‘Æ°á»£c

---

## ðŸŽ“ Káº¾T LUáº¬N

> **"Qua project nÃ y, em Ä‘Ã£ build má»™t complete end-to-end big data pipeline 
> xá»­ lÃ½ real-time data vá»›i Spark Streaming, Kafka, vÃ  MongoDB trÃªn Kubernetes. 
> Em hiá»ƒu sÃ¢u hÆ¡n vá» distributed computing, stream processing, vÃ  data engineering principles. 
> Em tin ráº±ng kiáº¿n thá»©c nÃ y ráº¥t valuable cho career trong data engineering."**

**CHÃšC Báº N DEMO THÃ€NH CÃ”NG! ðŸŽ‰**
