apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  namespace: default
data:
  # Spark Configuration
  spark.executor.memory: "2g"
  spark.driver.memory: "2g"
  spark.executor.cores: "2"
  spark.sql.shuffle.partitions: "8"
  
  # MongoDB Configuration
  mongodb.uri: "mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/"
  mongodb.database: "bigdata_db"
  
  # Kafka Configuration
  kafka.bootstrap.servers: "my-cluster-kafka-bootstrap.default.svc.cluster.local:9092"
  kafka.topic: "customer-events"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-scripts
  namespace: default
data:
  run-streaming-advanced.sh: |
    #!/bin/bash
    echo "Starting Advanced Streaming Application..."
    /opt/spark/bin/spark-submit \
      --master local[*] \
      --conf spark.mongodb.write.connection.uri="mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/" \
      --conf spark.mongodb.write.database="bigdata_db" \
      --conf spark.sql.streaming.checkpointLocation="/opt/spark/work-dir/checkpoints" \
      /opt/spark/work-dir/app/jobs/streaming/advanced_streaming.py
  
  # run-batch-ml.sh: |
  #   #!/bin/bash
  #   echo "Starting Batch ML Analytics..."
  #   /opt/spark/bin/spark-submit \
  #     --master local[*] \
  #     --conf spark.mongodb.read.connection.uri="mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/" \
  #     --conf spark.mongodb.write.connection.uri="mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/" \
  #     --conf spark.mongodb.read.database="bigdata_db" \
  #     --conf spark.mongodb.write.database="bigdata_db" \
  #     /opt/spark/work-dir/batch_analytics_ml.py

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-streaming-with-pvc
  namespace: default
  labels:
    app: spark-streaming-pvc
    tier: processing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-streaming-pvc
  template:
    metadata:
      labels:
        app: spark-streaming-pvc
        tier: processing
    spec:
      containers:
      - name: spark-streaming
        image: bigdata-spark:latest
        imagePullPolicy: Never
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "Waiting for dependencies..."
            sleep 30
            echo "Starting Streaming with PVC"
            /opt/spark/bin/spark-submit \
              --master local[*] \
              --driver-memory 1g \
              --executor-memory 1g \
              --conf spark.mongodb.write.connection.uri=mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/ \
              --conf spark.mongodb.write.database=bigdata_db \
              --conf spark.sql.streaming.checkpointLocation=/checkpoints \
              --conf spark.sql.shuffle.partitions=8 \
              --conf spark.sql.streaming.statefulOperator.checkCorrectness.enabled=false \
              /opt/spark/work-dir/app/jobs/streaming/advanced_streaming.py
        resources:
          requests:
            memory: "1Gi"
            cpu: "200m"
          limits:
            memory: "2Gi"
            cpu: "500m"
        env:
        - name: KAFKA_TOPIC
          value: "customer-events"
        - name: SPARK_LOCAL_DIRS
          value: /tmp/spark-local
        - name: SPARK_WORKER_DIR
          value: /opt/spark/work-dir
        volumeMounts:
        - name: checkpoints
          mountPath: /checkpoints
        - name: spark-local
          mountPath: /tmp/spark-local
      volumes:
      - name: checkpoints
        persistentVolumeClaim:
          claimName: spark-checkpoints-pvc
      - name: spark-local
        emptyDir: {}
      restartPolicy: Always
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: spark-checkpoints-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: Service
metadata:
  name: spark-streaming-svc
  namespace: default
spec:
  selector:
    app: spark-streaming-pvc
  ports:
  - name: spark-ui
    port: 4040
    targetPort: 4040
  type: ClusterIP

---
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: spark-batch-ml
#   namespace: default
#   labels:
#     app: spark-batch
#     tier: analytics
# spec:
#   ttlSecondsAfterFinished: 3600  # Clean up after 1 hour
#   backoffLimit: 3
#   template:
#     metadata:
#       labels:
#         app: spark-batch
#         tier: analytics
#     spec:
#       restartPolicy: OnFailure
#       containers:
#       - name: spark-batch
#         image: bigdata-spark:latest
#         imagePullPolicy: Never
#         command: ["/bin/bash", "-c"]
#         args:
#           - |
#             echo "Waiting for data to be available..."
#             sleep 60
#             echo "Starting Batch ML Analytics"
#             /opt/spark/bin/spark-submit \
#               --master local[*] \
#               --driver-memory 1g \
#               --executor-memory 1g \
#               --conf spark.mongodb.read.connection.uri=mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/ \
#               --conf spark.mongodb.write.connection.uri=mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/ \
#               --conf spark.mongodb.read.database=bigdata_db \
#               --conf spark.mongodb.write.database=bigdata_db \
#               --conf spark.sql.shuffle.partitions=8 \
#               --conf spark.sql.adaptive.enabled=true \
#               /opt/spark/work-dir/app/jobs/batch/ml_analytics.py
#         resources:
#           requests:
#             memory: "1Gi"
#             cpu: "200m"
#           limits:
#             memory: "2Gi"
#             cpu: "500m"
#         env:
#         - name: SPARK_LOCAL_DIRS
#           value: /tmp/spark-local
#         volumeMounts:
#         - name: spark-local
#           mountPath: /tmp/spark-local
#       volumes:
#       - name: spark-local
#         emptyDir: {}

# ---
# apiVersion: batch/v1
# kind: CronJob
# metadata:
#   name: spark-batch-ml-scheduled
#   namespace: default
# spec:
#   # Run every 6 hours
#   schedule: "0 */6 * * *"
#   jobTemplate:
#     spec:
#       ttlSecondsAfterFinished: 3600
#       template:
#         metadata:
#           labels:
#             app: spark-batch-scheduled
#         spec:
#           restartPolicy: OnFailure
#           containers:
#           - name: spark-batch
#             image: bigdata-spark:latest
#             imagePullPolicy: Never
#             command: ["/bin/bash", "-c"]
#             args:
#               - |
#                 echo "Starting Scheduled Batch ML Analytics"
#                 /opt/spark/bin/spark-submit \
#                   --master local[*] \
#                   --driver-memory 1g \
#                   --executor-memory 1g \
#                   --conf spark.mongodb.read.connection.uri=mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/ \
#                   --conf spark.mongodb.write.connection.uri=mongodb://my-mongo-mongodb.default.svc.cluster.local:27017/ \
#                   /opt/spark/work-dir/app/jobs/batch/ml_analytics.py
#             resources:
#               requests:
#                 memory: "1Gi"
#                 cpu: "200m"
#               limits:
#                 memory: "2Gi"
#                 cpu: "500m"


